{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#### As per the course policy, Tony Huang kindly helped me finding a BUG in my code.\n",
    "## Many thanks to Tony\n",
    "\n",
    "\n",
    "#### In addition, I referenced the original code at https://github.com/salesforce/awd-lstm-lm\n",
    "# This is allowed per the piazza post: https://piazza.com/class/khtqzctrciu1fp?cid=1217\n",
    "\n",
    "from torch.utils.data.dataset import T_co\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tests import test_prediction, test_generation, array_to_str\n",
    "\n",
    "SEQ_LENGTH = 70  # VARIABLE\n",
    "\n",
    "device = torch.device(0)\n",
    "\n",
    "# %%\n",
    "\n",
    "# load all that we need\n",
    "\n",
    "dataset = np.load('../dataset/wiki.train.npy', allow_pickle=True)  # [[int,...],...]\n",
    "devset = np.load('../dataset/wiki.valid.npy', allow_pickle=True)  # [[int,...],...]\n",
    "fixtures_pred = np.load('../fixtures/prediction.npz')  # dev\n",
    "fixtures_gen = np.load('../fixtures/generation.npy')  # dev\n",
    "fixtures_pred_test = np.load('../fixtures/prediction_test.npz')  # test\n",
    "fixtures_gen_test = np.load('../fixtures/generation_test.npy')  # test\n",
    "vocab = np.load('../dataset/vocab.npy')  # [str,...]\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "class LanguageModelSet(Dataset):\n",
    "\n",
    "    def __init__(self, data_loaded):\n",
    "        super().__init__()\n",
    "        data = torch.from_numpy(np.concatenate(data_loaded))\n",
    "\n",
    "        self.len = (data.shape[0] - 1) // SEQ_LENGTH\n",
    "\n",
    "        self.input = torch.zeros((self.len, SEQ_LENGTH), dtype=torch.long)\n",
    "        self.target = torch.zeros_like(self.input)\n",
    "\n",
    "        for i in range(self.len):\n",
    "            self.input[i] = data[i * SEQ_LENGTH:(i + 1) * SEQ_LENGTH]\n",
    "            self.target[i] = data[i * SEQ_LENGTH + 1:(i + 1) * SEQ_LENGTH + 1]\n",
    "\n",
    "    def __getitem__(self, index) -> T_co:\n",
    "        return self.input[index], self.target[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# data loader\n",
    "\n",
    "class LanguageModelDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        if isinstance(dataset, LanguageModelSet):\n",
    "            super(LanguageModelDataLoader, self).__init__(dataset, batch_size,\n",
    "                                                          shuffle)\n",
    "        else:\n",
    "            super(LanguageModelDataLoader, self).__init__(LanguageModelSet(dataset), batch_size,\n",
    "                                                          shuffle)\n",
    "\n",
    "\n",
    "# class LanguageModelDataLoader(DataLoader):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#\n",
    "#     def __init__(self, dataset, batch_size, shuffle=True):\n",
    "#         self.dataset = dataset\n",
    "#         self.batch_size = batch_size\n",
    "#\n",
    "#     def __iter__(self):\n",
    "#         data = torch.from_numpy(np.concatenate(self.dataset))\n",
    "#         self.len = (data.shape[0] - 1) // SEQ_LENGTH\n",
    "#         self.input = torch.zeros((self.len, SEQ_LENGTH), dtype=torch.long)\n",
    "#         self.target = torch.zeros_like(self.input)\n",
    "#\n",
    "#         for i in range(self.len):\n",
    "#             self.input[i] = data[i * SEQ_LENGTH:(i + 1) * SEQ_LENGTH]\n",
    "#             self.target[i] = data[i * SEQ_LENGTH + 1:(i + 1) * SEQ_LENGTH + 1]\n",
    "#\n",
    "#         for batch in range(self.len // self.batch_size):\n",
    "#             yield (self.input[batch * self.batch_size:(batch + 1) * self.batch_size, :],\n",
    "#                    self.target[batch * self.batch_size:(batch + 1) * self.batch_size, :])\n",
    "\n",
    "\n",
    "class LockedDropOut(nn.Module):\n",
    "    def __init__(self, T_dim=0):\n",
    "        super().__init__()\n",
    "        self.T_dim = T_dim\n",
    "\n",
    "    def forward(self, x, p):\n",
    "        \"\"\"\n",
    "        :param x: (T,B,C) or (B,T,C); T dimension is specified\n",
    "        :param p: probability\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if not self.training:\n",
    "            return x\n",
    "        if self.T_dim == 0:\n",
    "            mask = torch.zeros((1, x.shape[1], x.shape[2]), requires_grad=False,\n",
    "                               device=x.device).bernoulli_(1 - p)\n",
    "        elif self.T_dim == 1:\n",
    "            mask = torch.zeros((x.shape[0], 1, x.shape[2]), requires_grad=False,\n",
    "                               device=x.device).bernoulli_(1 - p)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        mask /= (1 - p)\n",
    "        mask = mask.expand_as(x)\n",
    "        return mask * x\n",
    "\n",
    "\n",
    "EMBEDDING_SIZE = 400\n",
    "HIDDEN_SIZE = 1150\n",
    "\n",
    "\n",
    "class WeightDrop(nn.Module):\n",
    "    def __init__(self, module, p=0.5):\n",
    "        super(WeightDrop, self).__init__()\n",
    "        self.p = p\n",
    "        self.module = module\n",
    "        self._setup()\n",
    "\n",
    "    def null(*args, **kwargs):\n",
    "        return\n",
    "\n",
    "    def _setup(self):\n",
    "        if issubclass(type(self.module), torch.nn.RNNBase):\n",
    "            self.module.flatten_parameters = self.null\n",
    "\n",
    "        w = getattr(self.module, 'weight_hh_l0')\n",
    "        del self.module._parameters['weight_hh_l0']\n",
    "        self.module.register_parameter('weight_hh_l0' + '_raw', nn.Parameter(w.data))\n",
    "\n",
    "    def _setweights(self):\n",
    "        raw_w = getattr(self.module, 'weight_hh_l0' + '_raw')\n",
    "        w = nn.Parameter(functional.dropout(raw_w, p=self.p, training=self.training))\n",
    "        setattr(self.module, 'weight_hh_l0', w)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        return self.module.forward(*args)\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n",
    "\n",
    "        self.d0 = LockedDropOut()\n",
    "        # self.r1 = nn.LSTM(EMBEDDING_SIZE, HIDDEN_SIZE)\n",
    "        # self.r2 = nn.LSTM(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        # self.r3 = nn.LSTM(HIDDEN_SIZE, EMBEDDING_SIZE)\n",
    "        self.r1 = WeightDrop(nn.LSTM(EMBEDDING_SIZE, HIDDEN_SIZE))\n",
    "        self.r2 = WeightDrop(nn.LSTM(HIDDEN_SIZE, HIDDEN_SIZE))\n",
    "        self.r3 = WeightDrop(nn.LSTM(HIDDEN_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "        self.linear = nn.Linear(EMBEDDING_SIZE, vocab_size)\n",
    "        self.linear.weight = self.embedding.weight\n",
    "\n",
    "        for weight in self.parameters():\n",
    "            nn.init.uniform_(weight, -1 / np.sqrt(HIDDEN_SIZE), 1 / np.sqrt(HIDDEN_SIZE))  # comment\n",
    "\n",
    "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)  # linear\n",
    "\n",
    "    def forward(self, x, hs=None, cs=None):\n",
    "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
    "        # x: (B,SEQ)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # (B,T,EMBEDDING)\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        # (T,B,Embedding)\n",
    "\n",
    "        if hs is not None:\n",
    "            x, (h1, c1) = self.r1(self.d0(x, 0.65), (hs[0], cs[0]))\n",
    "            x, (h2, c2) = self.r2(self.d0(x, 0.3), (hs[1], cs[1]))\n",
    "            x, (h3, c3) = self.r3(self.d0(x, 0.3), (hs[2], cs[2]))\n",
    "        else:\n",
    "            x, (h1, c1) = self.r1(self.d0(x, 0.65))\n",
    "            x, (h2, c2) = self.r2(self.d0(x, 0.3))\n",
    "            x, (h3, c3) = self.r3(self.d0(x, 0.3))\n",
    "\n",
    "        x = self.d0(x, 0.4)  # (T,B,E)\n",
    "        x = torch.transpose(x, 0, 1)  # (B,T,E)\n",
    "\n",
    "        # x = self.linear(x)  # (B,T,VOCAB)\n",
    "        # x = torch.reshape(x, (x.shape[0] * x.shape[1], -1))\n",
    "        # return x, ((h1, h2, h3), (c1, c2, c3))  # (B*T, VOCAB),(h,c)\n",
    "\n",
    "        out = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2]))\n",
    "\n",
    "        out = self.linear(out)  # (B*T,VOCAB)\n",
    "\n",
    "        x = torch.reshape(out, (x.shape[0], x.shape[1], -1))  # (B,T,VOCAB)\n",
    "\n",
    "        return torch.transpose(x, 1, 2), ((h1, h2, h3), (c1, c2, c3))  # (B, VOCAB, SEQ),(h,c)\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "# model trainer\n",
    "\n",
    "class LanguageModelTrainer:\n",
    "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
    "        \"\"\"\n",
    "            Use this class to train your model\n",
    "        \"\"\"\n",
    "        # feel free to add any other parameters here\n",
    "        self.model = model.cuda(device=device)\n",
    "        self.loader = loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.predictions = []\n",
    "        self.predictions_test = []\n",
    "        self.generated_logits = []\n",
    "        self.generated = []\n",
    "        self.generated_logits_test = []\n",
    "        self.generated_test = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.criterion = nn.CrossEntropyLoss().cuda(device=device)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()  # set to training mode\n",
    "        epoch_loss = 0\n",
    "        batch_num = 0\n",
    "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
    "            epoch_loss += self.train_batch(inputs, targets)\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        self.epochs += 1\n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
    "              % (self.epochs, self.max_epochs, epoch_loss))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "    def train_batch(self, inputs, targets):\n",
    "        output = self.model(inputs.to(device))[0]  # (reshape to flatten)\n",
    "\n",
    "        loss = self.criterion(output, targets.to(device))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def test(self):\n",
    "        # don't change these\n",
    "        self.model.eval()  # set to eval mode\n",
    "\n",
    "        # print(array_to_str(fixtures_pred['inp'], vocab))\n",
    "\n",
    "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'],\n",
    "                                                   self.model)  # get predictions\n",
    "        self.predictions.append(predictions)\n",
    "\n",
    "        # ####\n",
    "        #\n",
    "        # x = next(iter(self.loader))[0]\n",
    "        # gx = TestLanguageModel.generation(x, 10, self.model)\n",
    "        #\n",
    "        # ####\n",
    "\n",
    "        generated_logits = TestLanguageModel.generation(fixtures_gen, 10,\n",
    "                                                        self.model)  # generated predictions for\n",
    "        # 10 words\n",
    "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 10, self.model)\n",
    "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
    "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
    "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
    "        self.val_losses.append(nll)\n",
    "\n",
    "        self.generated.append(generated)\n",
    "        self.generated_test.append(generated_test)\n",
    "        self.generated_logits.append(generated_logits)\n",
    "        self.generated_logits_test.append(generated_logits_test)\n",
    "\n",
    "        # generate predictions for test data\n",
    "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'],\n",
    "                                                        self.model)  # get predictions\n",
    "        self.predictions_test.append(predictions_test)\n",
    "\n",
    "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
    "              % (self.epochs, self.max_epochs, nll))\n",
    "        return nll\n",
    "\n",
    "    def save(self):\n",
    "        # don't change these\n",
    "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
    "        torch.save({'state_dict': self.model.state_dict()},\n",
    "                   model_path)\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)),\n",
    "                self.predictions[-1])\n",
    "        np.save(\n",
    "                os.path.join('experiments', self.run_id,\n",
    "                             'predictions-test-{}.npy'.format(self.epochs)),\n",
    "                self.predictions_test[-1])\n",
    "        np.save(\n",
    "                os.path.join('experiments', self.run_id,\n",
    "                             'generated_logits-{}.npy'.format(self.epochs)),\n",
    "                self.generated_logits[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id,\n",
    "                             'generated_logits-test-{}.npy'.format(self.epochs)),\n",
    "                self.generated_logits_test[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)),\n",
    "                  'w') as fw:\n",
    "            fw.write(self.generated[-1])\n",
    "        with open(os.path.join('experiments', self.run_id,\n",
    "                               'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated_test[-1])\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "class TestLanguageModel:\n",
    "    @staticmethod\n",
    "    def prediction(inp, model):\n",
    "        inp = torch.from_numpy(inp)\n",
    "        inp = inp.to(device)\n",
    "\n",
    "        return model(inp)[0][:, :, -1].cpu().detach().numpy()\n",
    "\n",
    "    @staticmethod\n",
    "    def generation(inp, forward, model):\n",
    "        \"\"\"\n",
    "\n",
    "            Generate a sequence of words given a starting sequence.\n",
    "            :param model:\n",
    "            :param inp: Initial sequence of words (batch size, length)\n",
    "            :param forward: number of additional words to generate\n",
    "            :return: generated words (batch size, forward)\n",
    "        \"\"\"\n",
    "\n",
    "        inp = torch.from_numpy(inp)  # (B,T)\n",
    "        inp = inp.to(device)\n",
    "\n",
    "        result = torch.zeros((inp.shape[0], forward), device=inp.device, dtype=torch.long)\n",
    "\n",
    "        # res = model(inp)[:, :, -1]\n",
    "        # out = model(inp)\n",
    "        output, (hs, cs) = model(inp)\n",
    "        result[:, 0] = torch.argmax(output[:, :, -1], 1)  # (B,)\n",
    "        for i in range(1, forward):\n",
    "            inp = torch.unsqueeze(result[:, i - 1], 1)\n",
    "            output, (hs, cs) = model(inp, hs, cs)\n",
    "            result[:, i] = torch.argmax(output[:, :, -1], 1)\n",
    "\n",
    "        return result.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# %%\n",
    "\n",
    "run_id = str(int(time.time()))\n",
    "if not os.path.exists('./experiments'):\n",
    "    os.mkdir('./experiments')\n",
    "os.mkdir('./experiments/%s' % run_id)\n",
    "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)\n",
    "\n",
    "# %%\n",
    "\n",
    "model = LanguageModel(vocab.shape[0])\n",
    "dataset_torch = LanguageModelSet(dataset)\n",
    "loader = LanguageModelDataLoader(dataset=dataset_torch, batch_size=BATCH_SIZE, shuffle=True)\n",
    "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)\n",
    "\n",
    "# print(array_to_str(fixtures_pred['inp'][0], vocab))\n",
    "\n",
    "# %%\n",
    "\n",
    "best_nll = 1e30\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    trainer.train()\n",
    "    nll = trainer.test()\n",
    "    if nll < best_nll:\n",
    "        best_nll = nll\n",
    "        print(\"Saving model, predictions and generated output for epoch \" + str(\n",
    "                epoch) + \" with NLL: \" + str(best_nll))\n",
    "        trainer.save()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Don't change these\n",
    "# plot training curves\n",
    "plt.figure()\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('NLL')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "\n",
    "# see generated output\n",
    "print(trainer.generated[-1])  # get last generated output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}